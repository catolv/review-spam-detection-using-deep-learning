{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GRID K520 (CNMeM is disabled, cuDNN not available)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model variation is CNN-non-static\n",
      "Loading data...\n",
      "29235 words occure in pre-trained embeddings.\n",
      "Vocabulary Size: 33187\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train convolutional network for review spam detection. Based on\n",
    "https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras\n",
    "which is based on\n",
    "\"Convolutional Neural Networks for Sentence Classification\" by Yoon Kim\n",
    "http://arxiv.org/pdf/1408.5882v2.pdf\n",
    "(https://github.com/yoonkim/CNN_sentence)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "execfile(\"data_helpers.py\")\n",
    "# from w2v import train_word2vec ## not needed with pretrained vectors\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Input, Merge, Convolution1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "\n",
    "from sklearn import metrics\n",
    "import csv\n",
    "import cPickle as cpickle\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "model_variation = 'CNN-non-static'  ## CNN-rand | CNN-non-static | CNN-static\n",
    "print('Model variation is %s' % model_variation)\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 300\n",
    "filter_sizes = (3,4,5)\n",
    "num_filters = 50\n",
    "dropout_prob = (0.25, 0.5)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "num_epochs = 40\n",
    "val_split = 0.1\n",
    "\n",
    "# Paths to files\n",
    "data_name = \"100_v2\"\n",
    "pos_path = \"../CNN_sentence/spam_\" + data_name + \".txt\"\n",
    "neg_path = \"../CNN_sentence/ham_\" + data_name + \".txt\"\n",
    "test_path = \"../CNN_sentence/test_\" + data_name + \".txt\"\n",
    "test_labels_path = \"../CNN_sentence/test_results_\" + data_name + \".txt\"\n",
    "w2v_path = \"../Electronics_vectors300.bin\"\n",
    "\n",
    "# Output names\n",
    "history_name = \"history_100_f345_elec.p\"\n",
    "preds_name = \"preds_100_f345_elec.txt\"\n",
    "\n",
    "\n",
    "# Data Preparatopn\n",
    "# ==================================================\n",
    "#\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x, y, x_test, vocabulary, embeddings, sequence_length = load_data(pos_path, neg_path, test_path, w2v_path)\n",
    "\n",
    "if model_variation=='CNN-non-static' or model_variation=='CNN-static':\n",
    "    embedding_weights = [embeddings]\n",
    "    if model_variation=='CNN-static':\n",
    "        x = embedding_weights[0][x]\n",
    "        x_test = embedding_weights[0][x_test]\n",
    "elif model_variation=='CNN-rand':\n",
    "    embedding_weights = None\n",
    "else:\n",
    "    raise ValueError('Unknown model variation')\n",
    "\n",
    "# Shuffle data\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices].argmax(axis=1)\n",
    "\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model\n",
    "# ==================================================\n",
    "#\n",
    "# graph subnet with one input and one output,\n",
    "# convolutional layers concateneted in parallel\n",
    "graph_in = Input(shape=(sequence_length, embedding_dim))\n",
    "convs = []\n",
    "for fsz in filter_sizes:\n",
    "    conv = Convolution1D(nb_filter=num_filters,\n",
    "                         filter_length=fsz,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(graph_in)\n",
    "    pool = GlobalMaxPooling1D()(conv)\n",
    "    #flatten = Flatten()(pool) # ouput of pooling layer (None, 300) -> one value per sentence\n",
    "    convs.append(pool)\n",
    "    \n",
    "if len(filter_sizes)>1:\n",
    "    out = Merge(mode='concat')(convs)\n",
    "else:\n",
    "    out = convs[0]\n",
    "\n",
    "graph = Model(input=graph_in, output=out)\n",
    "\n",
    "# main sequential model\n",
    "model = Sequential()\n",
    "if not model_variation=='CNN-static':\n",
    "    model.add(Embedding(len(vocabulary)+1, embedding_dim, input_length=sequence_length,\n",
    "                        weights=embedding_weights))\n",
    "model.add(Dropout(dropout_prob[0], input_shape=(sequence_length, embedding_dim)))\n",
    "model.add(graph)\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(dropout_prob[1]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_test.shape)\n",
    "# print(x.shape)\n",
    "# print(embeddings.shape)\n",
    "# print(sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45266 samples, validate on 5030 samples\n",
      "Epoch 1/40\n"
     ]
    }
   ],
   "source": [
    "# Training model\n",
    "# ==================================================\n",
    "history = model.fit(x_shuffled, y_shuffled, batch_size=batch_size,\n",
    "          nb_epoch=40, validation_split=val_split, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and get performance\n",
    "# ==================================================\n",
    "y_test_pred = model.predict_classes(x_test, verbose = 0)\n",
    "\n",
    "with open(test_labels_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    y_test = np.asarray([int(r[0]) for r in reader])\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "print(cm)\n",
    "# Accuracy\n",
    "acc = metrics.accuracy_score(y_test, y_test_pred)\n",
    "print(acc)\n",
    "# F1 score\n",
    "f1 = metrics.f1_score(y_test, y_test_pred)\n",
    "print(f1)\n",
    "# AUC\n",
    "y_test_probs = model.predict_proba(x_test, verbose = 0) \n",
    "auc = metrics.roc_auc_score(y_test, y_test_probs)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training history graph and predictions\n",
    "with open(history_name, \"w\") as f:\n",
    "    cpickle.dump(history.history, f)\n",
    "\n",
    "all_preds = np.concatenate([y_test_pred, y_test_probs], axis = 1)\n",
    "\n",
    "with open(preds_name, \"w\") as f:\n",
    "    writer = csv.writer(f, delimiter = \";\")\n",
    "    for num in all_preds:\n",
    "        writer.writerow(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print training history\n",
    "# ==================================================\n",
    "\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "## summarize history for loss\n",
    "#plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "#plt.title('model loss')\n",
    "#plt.ylabel('loss')\n",
    "#plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'test'], loc='upper left')\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
